{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ; training loss: 0.689651191235\n",
      "Epoch: 1 ; training loss: 0.67684584856\n",
      "Epoch: 2 ; training loss: 0.667798817158\n",
      "Epoch: 3 ; training loss: 0.661276400089\n",
      "Epoch: 4 ; training loss: 0.656454563141\n",
      "Epoch: 5 ; training loss: 0.652784764767\n",
      "Epoch: 6 ; training loss: 0.649902403355\n",
      "Epoch: 7 ; training loss: 0.647563278675\n",
      "Epoch: 8 ; training loss: 0.645604610443\n",
      "Epoch: 9 ; training loss: 0.643916070461\n",
      "Epoch: 10 ; training loss: 0.642423331738\n",
      "Epoch: 11 ; training loss: 0.641075849533\n",
      "Epoch: 12 ; training loss: 0.639838159084\n",
      "Epoch: 13 ; training loss: 0.638686656952\n",
      "Epoch: 14 ; training loss: 0.637604296207\n",
      "Epoch: 15 ; training loss: 0.636579096317\n",
      "Epoch: 16 ; training loss: 0.635602533817\n",
      "Epoch: 17 ; training loss: 0.634668171406\n",
      "Epoch: 18 ; training loss: 0.633771598339\n",
      "Epoch: 19 ; training loss: 0.632908821106\n",
      "Epoch: 20 ; training loss: 0.632077336311\n",
      "Epoch: 21 ; training loss: 0.631274759769\n",
      "Epoch: 22 ; training loss: 0.630499243736\n",
      "Epoch: 23 ; training loss: 0.629749000072\n",
      "Epoch: 24 ; training loss: 0.629022717476\n",
      "Epoch: 25 ; training loss: 0.628319203854\n",
      "Epoch: 26 ; training loss: 0.627637267113\n",
      "Epoch: 27 ; training loss: 0.62697583437\n",
      "Epoch: 28 ; training loss: 0.626334130764\n",
      "Epoch: 29 ; training loss: 0.625711023808\n",
      "Epoch: 30 ; training loss: 0.625105559826\n",
      "Epoch: 31 ; training loss: 0.624517321587\n",
      "Epoch: 32 ; training loss: 0.623945355415\n",
      "Epoch: 33 ; training loss: 0.623388826847\n",
      "Epoch: 34 ; training loss: 0.622847378254\n",
      "Epoch: 35 ; training loss: 0.622320115566\n",
      "Epoch: 36 ; training loss: 0.621806263924\n",
      "Epoch: 37 ; training loss: 0.621305644512\n",
      "Epoch: 38 ; training loss: 0.620817422867\n",
      "Epoch: 39 ; training loss: 0.62034112215\n",
      "Epoch: 40 ; training loss: 0.61987644434\n",
      "Epoch: 41 ; training loss: 0.619422495365\n",
      "Epoch: 42 ; training loss: 0.618979215622\n",
      "Epoch: 43 ; training loss: 0.61854583025\n",
      "Epoch: 44 ; training loss: 0.618122220039\n",
      "Epoch: 45 ; training loss: 0.61770772934\n",
      "Epoch: 46 ; training loss: 0.617302119732\n",
      "Epoch: 47 ; training loss: 0.616905093193\n",
      "Epoch: 48 ; training loss: 0.616516053677\n",
      "Epoch: 49 ; training loss: 0.616134881973\n",
      "Epoch: 50 ; training loss: 0.615761220455\n",
      "Epoch: 51 ; training loss: 0.61539465189\n",
      "Epoch: 52 ; training loss: 0.615035057068\n",
      "Epoch: 53 ; training loss: 0.614682018757\n",
      "Epoch: 54 ; training loss: 0.614335358143\n",
      "Epoch: 55 ; training loss: 0.613994836807\n",
      "Epoch: 56 ; training loss: 0.613660216331\n",
      "Epoch: 57 ; training loss: 0.613331139088\n",
      "Epoch: 58 ; training loss: 0.613007485867\n",
      "Epoch: 59 ; training loss: 0.61268901825\n",
      "Epoch: 60 ; training loss: 0.612375676632\n",
      "Epoch: 61 ; training loss: 0.612067043781\n",
      "Epoch: 62 ; training loss: 0.611763060093\n",
      "Epoch: 63 ; training loss: 0.611463487148\n",
      "Epoch: 64 ; training loss: 0.611168265343\n",
      "Epoch: 65 ; training loss: 0.610877215862\n",
      "Epoch: 66 ; training loss: 0.610590159893\n",
      "Epoch: 67 ; training loss: 0.610306978226\n",
      "Epoch: 68 ; training loss: 0.610027313232\n",
      "Epoch: 69 ; training loss: 0.609751343727\n",
      "Epoch: 70 ; training loss: 0.609478831291\n",
      "Epoch: 71 ; training loss: 0.609209597111\n",
      "Epoch: 72 ; training loss: 0.608943641186\n",
      "Epoch: 73 ; training loss: 0.608680784702\n",
      "Epoch: 74 ; training loss: 0.608420729637\n",
      "Epoch: 75 ; training loss: 0.608163774014\n",
      "Epoch: 76 ; training loss: 0.607909500599\n",
      "Epoch: 77 ; training loss: 0.607657968998\n",
      "Epoch: 78 ; training loss: 0.607409000397\n",
      "Epoch: 79 ; training loss: 0.607162594795\n",
      "Epoch: 80 ; training loss: 0.60691857338\n",
      "Epoch: 81 ; training loss: 0.606677055359\n",
      "Epoch: 82 ; training loss: 0.60643774271\n",
      "Epoch: 83 ; training loss: 0.606200695038\n",
      "Epoch: 84 ; training loss: 0.605965733528\n",
      "Epoch: 85 ; training loss: 0.605732917786\n",
      "Epoch: 86 ; training loss: 0.605502188206\n",
      "Epoch: 87 ; training loss: 0.605273365974\n",
      "Epoch: 88 ; training loss: 0.605046451092\n",
      "Epoch: 89 ; training loss: 0.604821324348\n",
      "Epoch: 90 ; training loss: 0.604598104954\n",
      "Epoch: 91 ; training loss: 0.604376673698\n",
      "Epoch: 92 ; training loss: 0.604156911373\n",
      "Epoch: 93 ; training loss: 0.603938817978\n",
      "Epoch: 94 ; training loss: 0.603722274303\n",
      "Epoch: 95 ; training loss: 0.603507399559\n",
      "Epoch: 96 ; training loss: 0.603293955326\n",
      "Epoch: 97 ; training loss: 0.603082120419\n",
      "Epoch: 98 ; training loss: 0.602871775627\n",
      "Epoch: 99 ; training loss: 0.602662861347\n",
      "Epoch: 100 ; training loss: 0.602455198765\n",
      "Epoch: 101 ; training loss: 0.602249085903\n",
      "Epoch: 102 ; training loss: 0.602044045925\n",
      "Epoch: 103 ; training loss: 0.601840496063\n",
      "Epoch: 104 ; training loss: 0.60163807869\n",
      "Epoch: 105 ; training loss: 0.601437091827\n",
      "Epoch: 106 ; training loss: 0.601237237453\n",
      "Epoch: 107 ; training loss: 0.601038396358\n",
      "Epoch: 108 ; training loss: 0.60084104538\n",
      "Epoch: 109 ; training loss: 0.600644648075\n",
      "Epoch: 110 ; training loss: 0.600449323654\n",
      "Epoch: 111 ; training loss: 0.600255191326\n",
      "Epoch: 112 ; training loss: 0.600062191486\n",
      "Epoch: 113 ; training loss: 0.599870145321\n",
      "Epoch: 114 ; training loss: 0.599679112434\n",
      "Epoch: 115 ; training loss: 0.599489212036\n",
      "Epoch: 116 ; training loss: 0.599300205708\n",
      "Epoch: 117 ; training loss: 0.599112212658\n",
      "Epoch: 118 ; training loss: 0.598925232887\n",
      "Epoch: 119 ; training loss: 0.598739147186\n",
      "Epoch: 120 ; training loss: 0.59855401516\n",
      "Epoch: 121 ; training loss: 0.598369836807\n",
      "Epoch: 122 ; training loss: 0.598186612129\n",
      "Epoch: 123 ; training loss: 0.598004043102\n",
      "Epoch: 124 ; training loss: 0.597822487354\n",
      "Epoch: 125 ; training loss: 0.597641885281\n",
      "Epoch: 126 ; training loss: 0.597462058067\n",
      "Epoch: 127 ; training loss: 0.597283184528\n",
      "Epoch: 128 ; training loss: 0.59710508585\n",
      "Epoch: 129 ; training loss: 0.596927762032\n",
      "Epoch: 130 ; training loss: 0.596751213074\n",
      "Epoch: 131 ; training loss: 0.596575558186\n",
      "Epoch: 132 ; training loss: 0.596400618553\n",
      "Epoch: 133 ; training loss: 0.596226334572\n",
      "Epoch: 134 ; training loss: 0.596053183079\n",
      "Epoch: 135 ; training loss: 0.595880508423\n",
      "Epoch: 136 ; training loss: 0.595708727837\n",
      "Epoch: 137 ; training loss: 0.595537483692\n",
      "Epoch: 138 ; training loss: 0.595367193222\n",
      "Epoch: 139 ; training loss: 0.595197558403\n",
      "Epoch: 140 ; training loss: 0.595028579235\n",
      "Epoch: 141 ; training loss: 0.594860374928\n",
      "Epoch: 142 ; training loss: 0.59469294548\n",
      "Epoch: 143 ; training loss: 0.59452611208\n",
      "Epoch: 144 ; training loss: 0.594359993935\n",
      "Epoch: 145 ; training loss: 0.594194591045\n",
      "Epoch: 146 ; training loss: 0.594029903412\n",
      "Epoch: 147 ; training loss: 0.593865692616\n",
      "Epoch: 148 ; training loss: 0.593702495098\n",
      "Epoch: 149 ; training loss: 0.593539535999\n",
      "Epoch: 150 ; training loss: 0.59337747097\n",
      "Epoch: 151 ; training loss: 0.593216001987\n",
      "Epoch: 152 ; training loss: 0.593055188656\n",
      "Epoch: 153 ; training loss: 0.59289509058\n",
      "Epoch: 154 ; training loss: 0.592735469341\n",
      "Epoch: 155 ; training loss: 0.592576622963\n",
      "Epoch: 156 ; training loss: 0.592418372631\n",
      "Epoch: 157 ; training loss: 0.592260718346\n",
      "Epoch: 158 ; training loss: 0.592103660107\n",
      "Epoch: 159 ; training loss: 0.591947138309\n",
      "Epoch: 160 ; training loss: 0.591791391373\n",
      "Epoch: 161 ; training loss: 0.591636121273\n",
      "Epoch: 162 ; training loss: 0.591481387615\n",
      "Epoch: 163 ; training loss: 0.591327369213\n",
      "Epoch: 164 ; training loss: 0.591173946857\n",
      "Epoch: 165 ; training loss: 0.591021060944\n",
      "Epoch: 166 ; training loss: 0.590868771076\n",
      "Epoch: 167 ; training loss: 0.590717017651\n",
      "Epoch: 168 ; training loss: 0.590565979481\n",
      "Epoch: 169 ; training loss: 0.590415358543\n",
      "Epoch: 170 ; training loss: 0.590265274048\n",
      "Epoch: 171 ; training loss: 0.590115904808\n",
      "Epoch: 172 ; training loss: 0.589966952801\n",
      "Epoch: 173 ; training loss: 0.58981859684\n",
      "Epoch: 174 ; training loss: 0.589670777321\n",
      "Epoch: 175 ; training loss: 0.589523613453\n",
      "Epoch: 176 ; training loss: 0.589376807213\n",
      "Epoch: 177 ; training loss: 0.589230656624\n",
      "Epoch: 178 ; training loss: 0.589084982872\n",
      "Epoch: 179 ; training loss: 0.588939964771\n",
      "Epoch: 180 ; training loss: 0.588795363903\n",
      "Epoch: 181 ; training loss: 0.588651359081\n",
      "Epoch: 182 ; training loss: 0.588507890701\n",
      "Epoch: 183 ; training loss: 0.588364899158\n",
      "Epoch: 184 ; training loss: 0.588222444057\n",
      "Epoch: 185 ; training loss: 0.588080406189\n",
      "Epoch: 186 ; training loss: 0.587938904762\n",
      "Epoch: 187 ; training loss: 0.587798058987\n",
      "Epoch: 188 ; training loss: 0.587657630444\n",
      "Epoch: 189 ; training loss: 0.587517738342\n",
      "Epoch: 190 ; training loss: 0.587378323078\n",
      "Epoch: 191 ; training loss: 0.587239444256\n",
      "Epoch: 192 ; training loss: 0.587101042271\n",
      "Epoch: 193 ; training loss: 0.586963176727\n",
      "Epoch: 194 ; training loss: 0.586825668812\n",
      "Epoch: 195 ; training loss: 0.586688816547\n",
      "Epoch: 196 ; training loss: 0.586552381516\n",
      "Epoch: 197 ; training loss: 0.586416423321\n",
      "Epoch: 198 ; training loss: 0.586280882359\n",
      "Epoch: 199 ; training loss: 0.586145877838\n",
      "Epoch: 200 ; training loss: 0.586011469364\n",
      "Epoch: 201 ; training loss: 0.585877537727\n",
      "Epoch: 202 ; training loss: 0.585743844509\n",
      "Epoch: 203 ; training loss: 0.585610806942\n",
      "Epoch: 204 ; training loss: 0.585478186607\n",
      "Epoch: 205 ; training loss: 0.58534604311\n",
      "Epoch: 206 ; training loss: 0.58521425724\n",
      "Epoch: 207 ; training loss: 0.585083127022\n",
      "Epoch: 208 ; training loss: 0.584952354431\n",
      "Epoch: 209 ; training loss: 0.584821939468\n",
      "Epoch: 210 ; training loss: 0.584692180157\n",
      "Epoch: 211 ; training loss: 0.584562778473\n",
      "Epoch: 212 ; training loss: 0.584433853626\n",
      "Epoch: 213 ; training loss: 0.584305405617\n",
      "Epoch: 214 ; training loss: 0.58417737484\n",
      "Epoch: 215 ; training loss: 0.584049701691\n",
      "Epoch: 216 ; training loss: 0.583922564983\n",
      "Epoch: 217 ; training loss: 0.583795845509\n",
      "Epoch: 218 ; training loss: 0.583669543266\n",
      "Epoch: 219 ; training loss: 0.583543777466\n",
      "Epoch: 220 ; training loss: 0.583418250084\n",
      "Epoch: 221 ; training loss: 0.583293437958\n",
      "Epoch: 222 ; training loss: 0.583168745041\n",
      "Epoch: 223 ; training loss: 0.58304476738\n",
      "Epoch: 224 ; training loss: 0.582921028137\n",
      "Epoch: 225 ; training loss: 0.582797825336\n",
      "Epoch: 226 ; training loss: 0.582674920559\n",
      "Epoch: 227 ; training loss: 0.582552492619\n",
      "Epoch: 228 ; training loss: 0.582430481911\n",
      "Epoch: 229 ; training loss: 0.58230894804\n",
      "Epoch: 230 ; training loss: 0.582187831402\n",
      "Epoch: 231 ; training loss: 0.582067131996\n",
      "Epoch: 232 ; training loss: 0.581946790218\n",
      "Epoch: 233 ; training loss: 0.581826865673\n",
      "Epoch: 234 ; training loss: 0.581707298756\n",
      "Epoch: 235 ; training loss: 0.581588208675\n",
      "Epoch: 236 ; training loss: 0.581469476223\n",
      "Epoch: 237 ; training loss: 0.581351161003\n",
      "Epoch: 238 ; training loss: 0.58123332262\n",
      "Epoch: 239 ; training loss: 0.581115961075\n",
      "Epoch: 240 ; training loss: 0.580998778343\n",
      "Epoch: 241 ; training loss: 0.580882072449\n",
      "Epoch: 242 ; training loss: 0.580765843391\n",
      "Epoch: 243 ; training loss: 0.580649912357\n",
      "Epoch: 244 ; training loss: 0.580534338951\n",
      "Epoch: 245 ; training loss: 0.580419301987\n",
      "Epoch: 246 ; training loss: 0.580304503441\n",
      "Epoch: 247 ; training loss: 0.580190181732\n",
      "Epoch: 248 ; training loss: 0.580076217651\n",
      "Epoch: 249 ; training loss: 0.579962611198\n",
      "Epoch: 250 ; training loss: 0.579849421978\n",
      "Epoch: 251 ; training loss: 0.57973664999\n",
      "Epoch: 252 ; training loss: 0.579624176025\n",
      "Epoch: 253 ; training loss: 0.579512178898\n",
      "Epoch: 254 ; training loss: 0.579400479794\n",
      "Epoch: 255 ; training loss: 0.579289078712\n",
      "Epoch: 256 ; training loss: 0.579178154469\n",
      "Epoch: 257 ; training loss: 0.579067587852\n",
      "Epoch: 258 ; training loss: 0.578957438469\n",
      "Epoch: 259 ; training loss: 0.578847587109\n",
      "Epoch: 260 ; training loss: 0.578738033772\n",
      "Epoch: 261 ; training loss: 0.578629016876\n",
      "Epoch: 262 ; training loss: 0.578520298004\n",
      "Epoch: 263 ; training loss: 0.578411817551\n",
      "Epoch: 264 ; training loss: 0.578303813934\n",
      "Epoch: 265 ; training loss: 0.578196108341\n",
      "Epoch: 266 ; training loss: 0.578088879585\n",
      "Epoch: 267 ; training loss: 0.577981829643\n",
      "Epoch: 268 ; training loss: 0.577875196934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 ; training loss: 0.577769041061\n",
      "Epoch: 270 ; training loss: 0.577663183212\n",
      "Epoch: 271 ; training loss: 0.577557563782\n",
      "Epoch: 272 ; training loss: 0.577452540398\n",
      "Epoch: 273 ; training loss: 0.577347517014\n",
      "Epoch: 274 ; training loss: 0.577243030071\n",
      "Epoch: 275 ; training loss: 0.577138900757\n",
      "Epoch: 276 ; training loss: 0.577035069466\n",
      "Epoch: 277 ; training loss: 0.576931595802\n",
      "Epoch: 278 ; training loss: 0.576828360558\n",
      "Epoch: 279 ; training loss: 0.576725661755\n",
      "Epoch: 280 ; training loss: 0.576623082161\n",
      "Epoch: 281 ; training loss: 0.576521098614\n",
      "Epoch: 282 ; training loss: 0.576419174671\n",
      "Epoch: 283 ; training loss: 0.576317608356\n",
      "Epoch: 284 ; training loss: 0.576216697693\n",
      "Epoch: 285 ; training loss: 0.576115608215\n",
      "Epoch: 286 ; training loss: 0.576015293598\n",
      "Epoch: 287 ; training loss: 0.575914978981\n",
      "Epoch: 288 ; training loss: 0.575815320015\n",
      "Epoch: 289 ; training loss: 0.575715780258\n",
      "Epoch: 290 ; training loss: 0.575616598129\n",
      "Epoch: 291 ; training loss: 0.575517714024\n",
      "Epoch: 292 ; training loss: 0.575419127941\n",
      "Epoch: 293 ; training loss: 0.575320899487\n",
      "Epoch: 294 ; training loss: 0.57522302866\n",
      "Epoch: 295 ; training loss: 0.575125396252\n",
      "Epoch: 296 ; training loss: 0.575028121471\n",
      "Epoch: 297 ; training loss: 0.574931025505\n",
      "Epoch: 298 ; training loss: 0.574834406376\n",
      "Epoch: 299 ; training loss: 0.574738204479\n",
      "Epoch: 300 ; training loss: 0.574642062187\n",
      "Epoch: 301 ; training loss: 0.574546337128\n",
      "Epoch: 302 ; training loss: 0.574450850487\n",
      "Epoch: 303 ; training loss: 0.574355840683\n",
      "Epoch: 304 ; training loss: 0.574261009693\n",
      "Epoch: 305 ; training loss: 0.574166476727\n",
      "Epoch: 306 ; training loss: 0.574072241783\n",
      "Epoch: 307 ; training loss: 0.573978245258\n",
      "Epoch: 308 ; training loss: 0.573884725571\n",
      "Epoch: 309 ; training loss: 0.573791444302\n",
      "Epoch: 310 ; training loss: 0.573698401451\n",
      "Epoch: 311 ; training loss: 0.573605537415\n",
      "Epoch: 312 ; training loss: 0.57351320982\n",
      "Epoch: 313 ; training loss: 0.573421180248\n",
      "Epoch: 314 ; training loss: 0.573329210281\n",
      "Epoch: 315 ; training loss: 0.573237657547\n",
      "Epoch: 316 ; training loss: 0.573146402836\n",
      "Epoch: 317 ; training loss: 0.573055446148\n",
      "Epoch: 318 ; training loss: 0.572964727879\n",
      "Epoch: 319 ; training loss: 0.572874307632\n",
      "Epoch: 320 ; training loss: 0.572784245014\n",
      "Epoch: 321 ; training loss: 0.57269436121\n",
      "Epoch: 322 ; training loss: 0.572604835033\n",
      "Epoch: 323 ; training loss: 0.572515547276\n",
      "Epoch: 324 ; training loss: 0.572426557541\n",
      "Epoch: 325 ; training loss: 0.572337925434\n",
      "Epoch: 326 ; training loss: 0.572249352932\n",
      "Epoch: 327 ; training loss: 0.572161257267\n",
      "Epoch: 328 ; training loss: 0.572073400021\n",
      "Epoch: 329 ; training loss: 0.571985840797\n",
      "Epoch: 330 ; training loss: 0.571898341179\n",
      "Epoch: 331 ; training loss: 0.571811378002\n",
      "Epoch: 332 ; training loss: 0.571724534035\n",
      "Epoch: 333 ; training loss: 0.5716381073\n",
      "Epoch: 334 ; training loss: 0.571551799774\n",
      "Epoch: 335 ; training loss: 0.571465790272\n",
      "Epoch: 336 ; training loss: 0.571380138397\n",
      "Epoch: 337 ; training loss: 0.571294486523\n",
      "Epoch: 338 ; training loss: 0.571209430695\n",
      "Epoch: 339 ; training loss: 0.57112455368\n",
      "Epoch: 340 ; training loss: 0.57103985548\n",
      "Epoch: 341 ; training loss: 0.570955514908\n",
      "Epoch: 342 ; training loss: 0.570871412754\n",
      "Epoch: 343 ; training loss: 0.570787370205\n",
      "Epoch: 344 ; training loss: 0.570703864098\n",
      "Epoch: 345 ; training loss: 0.570620417595\n",
      "Epoch: 346 ; training loss: 0.57053732872\n",
      "Epoch: 347 ; training loss: 0.570454478264\n",
      "Epoch: 348 ; training loss: 0.570371925831\n",
      "Epoch: 349 ; training loss: 0.570289552212\n",
      "Epoch: 350 ; training loss: 0.570207417011\n",
      "Epoch: 351 ; training loss: 0.570125639439\n",
      "Epoch: 352 ; training loss: 0.570043921471\n",
      "Epoch: 353 ; training loss: 0.569962739944\n",
      "Epoch: 354 ; training loss: 0.569881558418\n",
      "Epoch: 355 ; training loss: 0.56980073452\n",
      "Epoch: 356 ; training loss: 0.56972014904\n",
      "Epoch: 357 ; training loss: 0.569639801979\n",
      "Epoch: 358 ; training loss: 0.569559693336\n",
      "Epoch: 359 ; training loss: 0.569479823112\n",
      "Epoch: 360 ; training loss: 0.569400191307\n",
      "Epoch: 361 ; training loss: 0.569320738316\n",
      "Epoch: 362 ; training loss: 0.569241583347\n",
      "Epoch: 363 ; training loss: 0.569162786007\n",
      "Epoch: 364 ; training loss: 0.569083929062\n",
      "Epoch: 365 ; training loss: 0.569005548954\n",
      "Epoch: 366 ; training loss: 0.56892734766\n",
      "Epoch: 367 ; training loss: 0.568849503994\n",
      "Epoch: 368 ; training loss: 0.568771719933\n",
      "Epoch: 369 ; training loss: 0.56869417429\n",
      "Epoch: 370 ; training loss: 0.568616986275\n",
      "Epoch: 371 ; training loss: 0.568539917469\n",
      "Epoch: 372 ; training loss: 0.568463087082\n",
      "Epoch: 373 ; training loss: 0.568386495113\n",
      "Epoch: 374 ; training loss: 0.568310201168\n",
      "Epoch: 375 ; training loss: 0.568234145641\n",
      "Epoch: 376 ; training loss: 0.568158149719\n",
      "Epoch: 377 ; training loss: 0.56808257103\n",
      "Epoch: 378 ; training loss: 0.568007171154\n",
      "Epoch: 379 ; training loss: 0.567931890488\n",
      "Epoch: 380 ; training loss: 0.567856907845\n",
      "Epoch: 381 ; training loss: 0.567782044411\n",
      "Epoch: 382 ; training loss: 0.567707657814\n",
      "Epoch: 383 ; training loss: 0.567633271217\n",
      "Epoch: 384 ; training loss: 0.567559242249\n",
      "Epoch: 385 ; training loss: 0.567485153675\n",
      "Epoch: 386 ; training loss: 0.567411601543\n",
      "Epoch: 387 ; training loss: 0.567338168621\n",
      "Epoch: 388 ; training loss: 0.567264974117\n",
      "Epoch: 389 ; training loss: 0.567191958427\n",
      "Epoch: 390 ; training loss: 0.567119121552\n",
      "Epoch: 391 ; training loss: 0.567046582699\n",
      "Epoch: 392 ; training loss: 0.566974282265\n",
      "Epoch: 393 ; training loss: 0.566902041435\n",
      "Epoch: 394 ; training loss: 0.566830158234\n",
      "Epoch: 395 ; training loss: 0.566758334637\n",
      "Epoch: 396 ; training loss: 0.566686809063\n",
      "Epoch: 397 ; training loss: 0.566615521908\n",
      "Epoch: 398 ; training loss: 0.566544413567\n",
      "Epoch: 399 ; training loss: 0.566473484039\n",
      "Epoch: 400 ; training loss: 0.566402852535\n",
      "Epoch: 401 ; training loss: 0.566332280636\n",
      "Epoch: 402 ; training loss: 0.566262125969\n",
      "Epoch: 403 ; training loss: 0.566191911697\n",
      "Epoch: 404 ; training loss: 0.566122114658\n",
      "Epoch: 405 ; training loss: 0.566052436829\n",
      "Epoch: 406 ; training loss: 0.565982997417\n",
      "Epoch: 407 ; training loss: 0.565913677216\n",
      "Epoch: 408 ; training loss: 0.565844595432\n",
      "Epoch: 409 ; training loss: 0.565775692463\n",
      "Epoch: 410 ; training loss: 0.565707027912\n",
      "Epoch: 411 ; training loss: 0.565638542175\n",
      "Epoch: 412 ; training loss: 0.565570175648\n",
      "Epoch: 413 ; training loss: 0.565502226353\n",
      "Epoch: 414 ; training loss: 0.565434336662\n",
      "Epoch: 415 ; training loss: 0.56536668539\n",
      "Epoch: 416 ; training loss: 0.565299093723\n",
      "Epoch: 417 ; training loss: 0.565231859684\n",
      "Epoch: 418 ; training loss: 0.565164685249\n",
      "Epoch: 419 ; training loss: 0.565097808838\n",
      "Epoch: 420 ; training loss: 0.56503111124\n",
      "Epoch: 421 ; training loss: 0.564964532852\n",
      "Epoch: 422 ; training loss: 0.564898252487\n",
      "Epoch: 423 ; training loss: 0.564832091331\n",
      "Epoch: 424 ; training loss: 0.56476598978\n",
      "Epoch: 425 ; training loss: 0.564700245857\n",
      "Epoch: 426 ; training loss: 0.564634680748\n",
      "Epoch: 427 ; training loss: 0.564569294453\n",
      "Epoch: 428 ; training loss: 0.564504086971\n",
      "Epoch: 429 ; training loss: 0.564439058304\n",
      "Epoch: 430 ; training loss: 0.564374148846\n",
      "Epoch: 431 ; training loss: 0.564309537411\n",
      "Epoch: 432 ; training loss: 0.564245045185\n",
      "Epoch: 433 ; training loss: 0.564180791378\n",
      "Epoch: 434 ; training loss: 0.564116537571\n",
      "Epoch: 435 ; training loss: 0.564052641392\n",
      "Epoch: 436 ; training loss: 0.563988924026\n",
      "Epoch: 437 ; training loss: 0.563925266266\n",
      "Epoch: 438 ; training loss: 0.563861787319\n",
      "Epoch: 439 ; training loss: 0.563798666\n",
      "Epoch: 440 ; training loss: 0.563735544682\n",
      "Epoch: 441 ; training loss: 0.563672721386\n",
      "Epoch: 442 ; training loss: 0.563610076904\n",
      "Epoch: 443 ; training loss: 0.563547432423\n",
      "Epoch: 444 ; training loss: 0.563485085964\n",
      "Epoch: 445 ; training loss: 0.563423037529\n",
      "Epoch: 446 ; training loss: 0.563361048698\n",
      "Epoch: 447 ; training loss: 0.563299059868\n",
      "Epoch: 448 ; training loss: 0.56323748827\n",
      "Epoch: 449 ; training loss: 0.563176035881\n",
      "Epoch: 450 ; training loss: 0.563114702702\n",
      "Epoch: 451 ; training loss: 0.563053429127\n",
      "Epoch: 452 ; training loss: 0.562992453575\n",
      "Epoch: 453 ; training loss: 0.562931716442\n",
      "Epoch: 454 ; training loss: 0.562871038914\n",
      "Epoch: 455 ; training loss: 0.562810599804\n",
      "Epoch: 456 ; training loss: 0.562750279903\n",
      "Epoch: 457 ; training loss: 0.562690138817\n",
      "Epoch: 458 ; training loss: 0.56263011694\n",
      "Epoch: 459 ; training loss: 0.562570393085\n",
      "Epoch: 460 ; training loss: 0.562510728836\n",
      "Epoch: 461 ; training loss: 0.562451243401\n",
      "Epoch: 462 ; training loss: 0.562391877174\n",
      "Epoch: 463 ; training loss: 0.562332689762\n",
      "Epoch: 464 ; training loss: 0.562273621559\n",
      "Epoch: 465 ; training loss: 0.562214910984\n",
      "Epoch: 466 ; training loss: 0.562156260014\n",
      "Epoch: 467 ; training loss: 0.562097668648\n",
      "Epoch: 468 ; training loss: 0.562039315701\n",
      "Epoch: 469 ; training loss: 0.561981141567\n",
      "Epoch: 470 ; training loss: 0.561923086643\n",
      "Epoch: 471 ; training loss: 0.561865150928\n",
      "Epoch: 472 ; training loss: 0.561807453632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 473 ; training loss: 0.561749875546\n",
      "Epoch: 474 ; training loss: 0.561692535877\n",
      "Epoch: 475 ; training loss: 0.561635255814\n",
      "Epoch: 476 ; training loss: 0.561578035355\n",
      "Epoch: 477 ; training loss: 0.561521232128\n",
      "Epoch: 478 ; training loss: 0.561464428902\n",
      "Epoch: 479 ; training loss: 0.561407804489\n",
      "Epoch: 480 ; training loss: 0.561351299286\n",
      "Epoch: 481 ; training loss: 0.561294794083\n",
      "Epoch: 482 ; training loss: 0.561238706112\n",
      "Epoch: 483 ; training loss: 0.56118273735\n",
      "Epoch: 484 ; training loss: 0.56112664938\n",
      "Epoch: 485 ; training loss: 0.561071038246\n",
      "Epoch: 486 ; training loss: 0.561015546322\n",
      "Epoch: 487 ; training loss: 0.560959994793\n",
      "Epoch: 488 ; training loss: 0.560904741287\n",
      "Epoch: 489 ; training loss: 0.560849547386\n",
      "Epoch: 490 ; training loss: 0.560794532299\n",
      "Epoch: 491 ; training loss: 0.560739696026\n",
      "Epoch: 492 ; training loss: 0.560685038567\n",
      "Epoch: 493 ; training loss: 0.560630381107\n",
      "Epoch: 494 ; training loss: 0.560576021671\n",
      "Epoch: 495 ; training loss: 0.56052172184\n",
      "Epoch: 496 ; training loss: 0.560467600822\n",
      "Epoch: 497 ; training loss: 0.56041353941\n",
      "Epoch: 498 ; training loss: 0.56035977602\n",
      "Epoch: 499 ; training loss: 0.560306072235\n",
      "Epoch: 500 ; training loss: 0.560252487659\n",
      "Epoch: 501 ; training loss: 0.560199081898\n",
      "Epoch: 502 ; training loss: 0.56014585495\n",
      "Epoch: 503 ; training loss: 0.560092568398\n",
      "Epoch: 504 ; training loss: 0.560039699078\n",
      "Epoch: 505 ; training loss: 0.559986770153\n",
      "Epoch: 506 ; training loss: 0.559934020042\n",
      "Epoch: 507 ; training loss: 0.55988150835\n",
      "Epoch: 508 ; training loss: 0.559828996658\n",
      "Epoch: 509 ; training loss: 0.559776723385\n",
      "Epoch: 510 ; training loss: 0.559724509716\n",
      "Epoch: 511 ; training loss: 0.559672474861\n",
      "Epoch: 512 ; training loss: 0.559620678425\n",
      "Epoch: 513 ; training loss: 0.559568822384\n",
      "Epoch: 514 ; training loss: 0.559517204762\n",
      "Epoch: 515 ; training loss: 0.559465646744\n",
      "Epoch: 516 ; training loss: 0.559414327145\n",
      "Epoch: 517 ; training loss: 0.55936306715\n",
      "Epoch: 518 ; training loss: 0.55931198597\n",
      "Epoch: 519 ; training loss: 0.559261023998\n",
      "Epoch: 520 ; training loss: 0.559210181236\n",
      "Epoch: 521 ; training loss: 0.559159457684\n",
      "Epoch: 522 ; training loss: 0.559108912945\n",
      "Epoch: 523 ; training loss: 0.559058487415\n",
      "Epoch: 524 ; training loss: 0.55900812149\n",
      "Epoch: 525 ; training loss: 0.55895793438\n",
      "Epoch: 526 ; training loss: 0.558907866478\n",
      "Epoch: 527 ; training loss: 0.558857917786\n",
      "Epoch: 528 ; training loss: 0.558808207512\n",
      "Epoch: 529 ; training loss: 0.558758437634\n",
      "Epoch: 530 ; training loss: 0.558708906174\n",
      "Epoch: 531 ; training loss: 0.558659434319\n",
      "Epoch: 532 ; training loss: 0.558610200882\n",
      "Epoch: 533 ; training loss: 0.55856102705\n",
      "Epoch: 534 ; training loss: 0.558511972427\n",
      "Epoch: 535 ; training loss: 0.558462977409\n",
      "Epoch: 536 ; training loss: 0.55841422081\n",
      "Epoch: 537 ; training loss: 0.55836558342\n",
      "Epoch: 538 ; training loss: 0.558317065239\n",
      "Epoch: 539 ; training loss: 0.558268547058\n",
      "Epoch: 540 ; training loss: 0.5582203269\n",
      "Epoch: 541 ; training loss: 0.558172106743\n",
      "Epoch: 542 ; training loss: 0.558124065399\n",
      "Epoch: 543 ; training loss: 0.558076143265\n",
      "Epoch: 544 ; training loss: 0.558028280735\n",
      "Epoch: 545 ; training loss: 0.557980597019\n",
      "Epoch: 546 ; training loss: 0.557933032513\n",
      "Epoch: 547 ; training loss: 0.557885587215\n",
      "Epoch: 548 ; training loss: 0.557838201523\n",
      "Epoch: 549 ; training loss: 0.557790994644\n",
      "Epoch: 550 ; training loss: 0.557743787766\n",
      "Epoch: 551 ; training loss: 0.55769687891\n",
      "Epoch: 552 ; training loss: 0.557650029659\n",
      "Epoch: 553 ; training loss: 0.557603299618\n",
      "Epoch: 554 ; training loss: 0.557556569576\n",
      "Epoch: 555 ; training loss: 0.557510077953\n",
      "Epoch: 556 ; training loss: 0.557463765144\n",
      "Epoch: 557 ; training loss: 0.557417452335\n",
      "Epoch: 558 ; training loss: 0.55737131834\n",
      "Epoch: 559 ; training loss: 0.557325184345\n",
      "Epoch: 560 ; training loss: 0.557279169559\n",
      "Epoch: 561 ; training loss: 0.557233333588\n",
      "Epoch: 562 ; training loss: 0.557187736034\n",
      "Epoch: 563 ; training loss: 0.557142078876\n",
      "Epoch: 564 ; training loss: 0.557096540928\n",
      "Epoch: 565 ; training loss: 0.557051241398\n",
      "Epoch: 566 ; training loss: 0.557005941868\n",
      "Epoch: 567 ; training loss: 0.556960821152\n",
      "Epoch: 568 ; training loss: 0.55691576004\n",
      "Epoch: 569 ; training loss: 0.556870758533\n",
      "Epoch: 570 ; training loss: 0.556825995445\n",
      "Epoch: 571 ; training loss: 0.556781291962\n",
      "Epoch: 572 ; training loss: 0.556736707687\n",
      "Epoch: 573 ; training loss: 0.556692183018\n",
      "Epoch: 574 ; training loss: 0.556647717953\n",
      "Epoch: 575 ; training loss: 0.556603491306\n",
      "Epoch: 576 ; training loss: 0.556559383869\n",
      "Epoch: 577 ; training loss: 0.556515216827\n",
      "Epoch: 578 ; training loss: 0.5564712286\n",
      "Epoch: 579 ; training loss: 0.556427419186\n",
      "Epoch: 580 ; training loss: 0.556383669376\n",
      "Epoch: 581 ; training loss: 0.556340038776\n",
      "Epoch: 582 ; training loss: 0.556296467781\n",
      "Epoch: 583 ; training loss: 0.556253135204\n",
      "Epoch: 584 ; training loss: 0.556209743023\n",
      "Epoch: 585 ; training loss: 0.556166470051\n",
      "Epoch: 586 ; training loss: 0.556123316288\n",
      "Epoch: 587 ; training loss: 0.556080460548\n",
      "Epoch: 588 ; training loss: 0.556037545204\n",
      "Epoch: 589 ; training loss: 0.55599462986\n",
      "Epoch: 590 ; training loss: 0.555951952934\n",
      "Epoch: 591 ; training loss: 0.555909395218\n",
      "Epoch: 592 ; training loss: 0.555866956711\n",
      "Epoch: 593 ; training loss: 0.555824458599\n",
      "Epoch: 594 ; training loss: 0.555782139301\n",
      "Epoch: 595 ; training loss: 0.555739939213\n",
      "Epoch: 596 ; training loss: 0.555697917938\n",
      "Epoch: 597 ; training loss: 0.555655896664\n",
      "Epoch: 598 ; training loss: 0.555613994598\n",
      "Epoch: 599 ; training loss: 0.555572271347\n",
      "Epoch: 600 ; training loss: 0.555530488491\n",
      "Epoch: 601 ; training loss: 0.555488944054\n",
      "Epoch: 602 ; training loss: 0.555447399616\n",
      "Epoch: 603 ; training loss: 0.555406033993\n",
      "Epoch: 604 ; training loss: 0.555364668369\n",
      "Epoch: 605 ; training loss: 0.555323600769\n",
      "Epoch: 606 ; training loss: 0.555282354355\n",
      "Epoch: 607 ; training loss: 0.555241286755\n",
      "Epoch: 608 ; training loss: 0.555200517178\n",
      "Epoch: 609 ; training loss: 0.555159747601\n",
      "Epoch: 610 ; training loss: 0.555119037628\n",
      "Epoch: 611 ; training loss: 0.555078327656\n",
      "Epoch: 612 ; training loss: 0.555037856102\n",
      "Epoch: 613 ; training loss: 0.554997324944\n",
      "Epoch: 614 ; training loss: 0.554957091808\n",
      "Epoch: 615 ; training loss: 0.554916858673\n",
      "Epoch: 616 ; training loss: 0.554876685143\n",
      "Epoch: 617 ; training loss: 0.554836571217\n",
      "Epoch: 618 ; training loss: 0.554796636105\n",
      "Epoch: 619 ; training loss: 0.554756700993\n",
      "Epoch: 620 ; training loss: 0.554716944695\n",
      "Epoch: 621 ; training loss: 0.554677307606\n",
      "Epoch: 622 ; training loss: 0.554637670517\n",
      "Epoch: 623 ; training loss: 0.554598212242\n",
      "Epoch: 624 ; training loss: 0.554558753967\n",
      "Epoch: 625 ; training loss: 0.554519414902\n",
      "Epoch: 626 ; training loss: 0.55448025465\n",
      "Epoch: 627 ; training loss: 0.554441034794\n",
      "Epoch: 628 ; training loss: 0.554402053356\n",
      "Epoch: 629 ; training loss: 0.554363071918\n",
      "Epoch: 630 ; training loss: 0.554324269295\n",
      "Epoch: 631 ; training loss: 0.554285466671\n",
      "Epoch: 632 ; training loss: 0.554246723652\n",
      "Epoch: 633 ; training loss: 0.554208219051\n",
      "Epoch: 634 ; training loss: 0.554169654846\n",
      "Epoch: 635 ; training loss: 0.554131150246\n",
      "Epoch: 636 ; training loss: 0.554092884064\n",
      "Epoch: 637 ; training loss: 0.554054677486\n",
      "Epoch: 638 ; training loss: 0.554016530514\n",
      "Epoch: 639 ; training loss: 0.55397850275\n",
      "Epoch: 640 ; training loss: 0.553940415382\n",
      "Epoch: 641 ; training loss: 0.553902506828\n",
      "Epoch: 642 ; training loss: 0.553864717484\n",
      "Epoch: 643 ; training loss: 0.553827106953\n",
      "Epoch: 644 ; training loss: 0.553789317608\n",
      "Epoch: 645 ; training loss: 0.553751766682\n",
      "Epoch: 646 ; training loss: 0.553714334965\n",
      "Epoch: 647 ; training loss: 0.553677022457\n",
      "Epoch: 648 ; training loss: 0.553639650345\n",
      "Epoch: 649 ; training loss: 0.553602457047\n",
      "Epoch: 650 ; training loss: 0.553565323353\n",
      "Epoch: 651 ; training loss: 0.553528249264\n",
      "Epoch: 652 ; training loss: 0.553491234779\n",
      "Epoch: 653 ; training loss: 0.553454399109\n",
      "Epoch: 654 ; training loss: 0.553417682648\n",
      "Epoch: 655 ; training loss: 0.553380846977\n",
      "Epoch: 656 ; training loss: 0.55334430933\n",
      "Epoch: 657 ; training loss: 0.553307712078\n",
      "Epoch: 658 ; training loss: 0.55327129364\n",
      "Epoch: 659 ; training loss: 0.553234755993\n",
      "Epoch: 660 ; training loss: 0.553198575974\n",
      "Epoch: 661 ; training loss: 0.553162336349\n",
      "Epoch: 662 ; training loss: 0.553126215935\n",
      "Epoch: 663 ; training loss: 0.55309009552\n",
      "Epoch: 664 ; training loss: 0.553054153919\n",
      "Epoch: 665 ; training loss: 0.553018212318\n",
      "Epoch: 666 ; training loss: 0.552982330322\n",
      "Epoch: 667 ; training loss: 0.55294662714\n",
      "Epoch: 668 ; training loss: 0.552911043167\n",
      "Epoch: 669 ; training loss: 0.552875339985\n",
      "Epoch: 670 ; training loss: 0.552839815617\n",
      "Epoch: 671 ; training loss: 0.552804470062\n",
      "Epoch: 672 ; training loss: 0.552769124508\n",
      "Epoch: 673 ; training loss: 0.552733778954\n",
      "Epoch: 674 ; training loss: 0.552698612213\n",
      "Epoch: 675 ; training loss: 0.552663505077\n",
      "Epoch: 676 ; training loss: 0.552628457546\n",
      "Epoch: 677 ; training loss: 0.552593410015\n",
      "Epoch: 678 ; training loss: 0.552558600903\n",
      "Epoch: 679 ; training loss: 0.552523851395\n",
      "Epoch: 680 ; training loss: 0.552488982677\n",
      "Epoch: 681 ; training loss: 0.552454411983\n",
      "Epoch: 682 ; training loss: 0.55241972208\n",
      "Epoch: 683 ; training loss: 0.5523853302\n",
      "Epoch: 684 ; training loss: 0.552350878716\n",
      "Epoch: 685 ; training loss: 0.55231654644\n",
      "Epoch: 686 ; training loss: 0.552282214165\n",
      "Epoch: 687 ; training loss: 0.552248001099\n",
      "Epoch: 688 ; training loss: 0.552213966846\n",
      "Epoch: 689 ; training loss: 0.552179813385\n",
      "Epoch: 690 ; training loss: 0.552145898342\n",
      "Epoch: 691 ; training loss: 0.552111923695\n",
      "Epoch: 692 ; training loss: 0.552078127861\n",
      "Epoch: 693 ; training loss: 0.552044332027\n",
      "Epoch: 694 ; training loss: 0.552010655403\n",
      "Epoch: 695 ; training loss: 0.551977038383\n",
      "Epoch: 696 ; training loss: 0.551943421364\n",
      "Epoch: 697 ; training loss: 0.551909923553\n",
      "Epoch: 698 ; training loss: 0.551876544952\n",
      "Epoch: 699 ; training loss: 0.551843285561\n",
      "Epoch: 700 ; training loss: 0.55180990696\n",
      "Epoch: 701 ; training loss: 0.551776707172\n",
      "Epoch: 702 ; training loss: 0.55174356699\n",
      "Epoch: 703 ; training loss: 0.551710665226\n",
      "Epoch: 704 ; training loss: 0.551677644253\n",
      "Epoch: 705 ; training loss: 0.551644682884\n",
      "Epoch: 706 ; training loss: 0.551611840725\n",
      "Epoch: 707 ; training loss: 0.55157905817\n",
      "Epoch: 708 ; training loss: 0.551546394825\n",
      "Epoch: 709 ; training loss: 0.551513850689\n",
      "Epoch: 710 ; training loss: 0.551481306553\n",
      "Epoch: 711 ; training loss: 0.551448702812\n",
      "Epoch: 712 ; training loss: 0.55141633749\n",
      "Epoch: 713 ; training loss: 0.551384031773\n",
      "Epoch: 714 ; training loss: 0.551351726055\n",
      "Epoch: 715 ; training loss: 0.551319479942\n",
      "Epoch: 716 ; training loss: 0.551287293434\n",
      "Epoch: 717 ; training loss: 0.55125528574\n",
      "Epoch: 718 ; training loss: 0.551223218441\n",
      "Epoch: 719 ; training loss: 0.551191270351\n",
      "Epoch: 720 ; training loss: 0.551159381866\n",
      "Epoch: 721 ; training loss: 0.551127612591\n",
      "Epoch: 722 ; training loss: 0.55109590292\n",
      "Epoch: 723 ; training loss: 0.551064193249\n",
      "Epoch: 724 ; training loss: 0.551032543182\n",
      "Epoch: 725 ; training loss: 0.55100107193\n",
      "Epoch: 726 ; training loss: 0.550969481468\n",
      "Epoch: 727 ; training loss: 0.55093818903\n",
      "Epoch: 728 ; training loss: 0.550906836987\n",
      "Epoch: 729 ; training loss: 0.550875484943\n",
      "Epoch: 730 ; training loss: 0.550844311714\n",
      "Epoch: 731 ; training loss: 0.550813019276\n",
      "Epoch: 732 ; training loss: 0.550781965256\n",
      "Epoch: 733 ; training loss: 0.55075097084\n",
      "Epoch: 734 ; training loss: 0.550719976425\n",
      "Epoch: 735 ; training loss: 0.550689160824\n",
      "Epoch: 736 ; training loss: 0.550658404827\n",
      "Epoch: 737 ; training loss: 0.550627470016\n",
      "Epoch: 738 ; training loss: 0.550596773624\n",
      "Epoch: 739 ; training loss: 0.550566196442\n",
      "Epoch: 740 ; training loss: 0.550535559654\n",
      "Epoch: 741 ; training loss: 0.550505101681\n",
      "Epoch: 742 ; training loss: 0.550474643707\n",
      "Epoch: 743 ; training loss: 0.550444304943\n",
      "Epoch: 744 ; training loss: 0.550413906574\n",
      "Epoch: 745 ; training loss: 0.550383627415\n",
      "Epoch: 746 ; training loss: 0.550353527069\n",
      "Epoch: 747 ; training loss: 0.550323307514\n",
      "Epoch: 748 ; training loss: 0.550293266773\n",
      "Epoch: 749 ; training loss: 0.550263285637\n",
      "Epoch: 750 ; training loss: 0.550233244896\n",
      "Epoch: 751 ; training loss: 0.550203382969\n",
      "Epoch: 752 ; training loss: 0.550173580647\n",
      "Epoch: 753 ; training loss: 0.550143718719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 754 ; training loss: 0.550113976002\n",
      "Epoch: 755 ; training loss: 0.550084352493\n",
      "Epoch: 756 ; training loss: 0.550054788589\n",
      "Epoch: 757 ; training loss: 0.550025224686\n",
      "Epoch: 758 ; training loss: 0.549995660782\n",
      "Epoch: 759 ; training loss: 0.549966335297\n",
      "Epoch: 760 ; training loss: 0.549937069416\n",
      "Epoch: 761 ; training loss: 0.549907803535\n",
      "Epoch: 762 ; training loss: 0.54987859726\n",
      "Epoch: 763 ; training loss: 0.549849331379\n",
      "Epoch: 764 ; training loss: 0.549820184708\n",
      "Epoch: 765 ; training loss: 0.549791097641\n",
      "Epoch: 766 ; training loss: 0.549762070179\n",
      "Epoch: 767 ; training loss: 0.549733161926\n",
      "Epoch: 768 ; training loss: 0.549704253674\n",
      "Epoch: 769 ; training loss: 0.54967546463\n",
      "Epoch: 770 ; training loss: 0.549646675587\n",
      "Epoch: 771 ; training loss: 0.549617946148\n",
      "Epoch: 772 ; training loss: 0.549589335918\n",
      "Epoch: 773 ; training loss: 0.549560666084\n",
      "Epoch: 774 ; training loss: 0.549532175064\n",
      "Epoch: 775 ; training loss: 0.549503684044\n",
      "Epoch: 776 ; training loss: 0.549475312233\n",
      "Epoch: 777 ; training loss: 0.549446940422\n",
      "Epoch: 778 ; training loss: 0.549418509007\n",
      "Epoch: 779 ; training loss: 0.549390375614\n",
      "Epoch: 780 ; training loss: 0.549362123013\n",
      "Epoch: 781 ; training loss: 0.549333870411\n",
      "Epoch: 782 ; training loss: 0.549305796623\n",
      "Epoch: 783 ; training loss: 0.549277842045\n",
      "Epoch: 784 ; training loss: 0.549249827862\n",
      "Epoch: 785 ; training loss: 0.549221873283\n",
      "Epoch: 786 ; training loss: 0.549194037914\n",
      "Epoch: 787 ; training loss: 0.54916626215\n",
      "Epoch: 788 ; training loss: 0.549138426781\n",
      "Epoch: 789 ; training loss: 0.549110770226\n",
      "Epoch: 790 ; training loss: 0.54908311367\n",
      "Epoch: 791 ; training loss: 0.54905551672\n",
      "Epoch: 792 ; training loss: 0.549028038979\n",
      "Epoch: 793 ; training loss: 0.549000442028\n",
      "Epoch: 794 ; training loss: 0.548973023891\n",
      "Epoch: 795 ; training loss: 0.548945605755\n",
      "Epoch: 796 ; training loss: 0.548918306828\n",
      "Epoch: 797 ; training loss: 0.5488910079\n",
      "Epoch: 798 ; training loss: 0.548863768578\n",
      "Epoch: 799 ; training loss: 0.548836648464\n",
      "Epoch: 800 ; training loss: 0.548809528351\n",
      "Epoch: 801 ; training loss: 0.548782408237\n",
      "Epoch: 802 ; training loss: 0.548755466938\n",
      "Epoch: 803 ; training loss: 0.548728466034\n",
      "Epoch: 804 ; training loss: 0.548701524734\n",
      "Epoch: 805 ; training loss: 0.54867464304\n",
      "Epoch: 806 ; training loss: 0.548647880554\n",
      "Epoch: 807 ; training loss: 0.548621237278\n",
      "Epoch: 808 ; training loss: 0.548594355583\n",
      "Epoch: 809 ; training loss: 0.548567771912\n",
      "Epoch: 810 ; training loss: 0.54854118824\n",
      "Epoch: 811 ; training loss: 0.548514604568\n",
      "Epoch: 812 ; training loss: 0.548488140106\n",
      "Epoch: 813 ; training loss: 0.548461675644\n",
      "Epoch: 814 ; training loss: 0.548435389996\n",
      "Epoch: 815 ; training loss: 0.548408925533\n",
      "Epoch: 816 ; training loss: 0.548382639885\n",
      "Epoch: 817 ; training loss: 0.548356473446\n",
      "Epoch: 818 ; training loss: 0.548330187798\n",
      "Epoch: 819 ; training loss: 0.548304021358\n",
      "Epoch: 820 ; training loss: 0.548277914524\n",
      "Epoch: 821 ; training loss: 0.548251926899\n",
      "Epoch: 822 ; training loss: 0.548225939274\n",
      "Epoch: 823 ; training loss: 0.548200011253\n",
      "Epoch: 824 ; training loss: 0.548174023628\n",
      "Epoch: 825 ; training loss: 0.548148214817\n",
      "Epoch: 826 ; training loss: 0.548122346401\n",
      "Epoch: 827 ; training loss: 0.548096716404\n",
      "Epoch: 828 ; training loss: 0.548070967197\n",
      "Epoch: 829 ; training loss: 0.548045277596\n",
      "Epoch: 830 ; training loss: 0.548019707203\n",
      "Epoch: 831 ; training loss: 0.547994196415\n",
      "Epoch: 832 ; training loss: 0.547968566418\n",
      "Epoch: 833 ; training loss: 0.547943174839\n",
      "Epoch: 834 ; training loss: 0.547917723656\n",
      "Epoch: 835 ; training loss: 0.547892391682\n",
      "Epoch: 836 ; training loss: 0.547867000103\n",
      "Epoch: 837 ; training loss: 0.547841846943\n",
      "Epoch: 838 ; training loss: 0.547816634178\n",
      "Epoch: 839 ; training loss: 0.547791421413\n",
      "Epoch: 840 ; training loss: 0.547766268253\n",
      "Epoch: 841 ; training loss: 0.547741174698\n",
      "Epoch: 842 ; training loss: 0.547716200352\n",
      "Epoch: 843 ; training loss: 0.54769128561\n",
      "Epoch: 844 ; training loss: 0.547666192055\n",
      "Epoch: 845 ; training loss: 0.547641396523\n",
      "Epoch: 846 ; training loss: 0.547616422176\n",
      "Epoch: 847 ; training loss: 0.547591745853\n",
      "Epoch: 848 ; training loss: 0.547566950321\n",
      "Epoch: 849 ; training loss: 0.547542273998\n",
      "Epoch: 850 ; training loss: 0.547517478466\n",
      "Epoch: 851 ; training loss: 0.547492921352\n",
      "Epoch: 852 ; training loss: 0.547468364239\n",
      "Epoch: 853 ; training loss: 0.547443807125\n",
      "Epoch: 854 ; training loss: 0.547419369221\n",
      "Epoch: 855 ; training loss: 0.547394871712\n",
      "Epoch: 856 ; training loss: 0.547370433807\n",
      "Epoch: 857 ; training loss: 0.547346174717\n",
      "Epoch: 858 ; training loss: 0.547321915627\n",
      "Epoch: 859 ; training loss: 0.547297596931\n",
      "Epoch: 860 ; training loss: 0.54727345705\n",
      "Epoch: 861 ; training loss: 0.547249078751\n",
      "Epoch: 862 ; training loss: 0.547225058079\n",
      "Epoch: 863 ; training loss: 0.547200977802\n",
      "Epoch: 864 ; training loss: 0.54717695713\n",
      "Epoch: 865 ; training loss: 0.547152996063\n",
      "Epoch: 866 ; training loss: 0.547128975391\n",
      "Epoch: 867 ; training loss: 0.547105073929\n",
      "Epoch: 868 ; training loss: 0.547081172466\n",
      "Epoch: 869 ; training loss: 0.547057449818\n",
      "Epoch: 870 ; training loss: 0.547033548355\n",
      "Epoch: 871 ; training loss: 0.547009825706\n",
      "Epoch: 872 ; training loss: 0.546986162663\n",
      "Epoch: 873 ; training loss: 0.546962440014\n",
      "Epoch: 874 ; training loss: 0.546938896179\n",
      "Epoch: 875 ; training loss: 0.546915352345\n",
      "Epoch: 876 ; training loss: 0.546891868114\n",
      "Epoch: 877 ; training loss: 0.54686832428\n",
      "Epoch: 878 ; training loss: 0.546844899654\n",
      "Epoch: 879 ; training loss: 0.546821534634\n",
      "Epoch: 880 ; training loss: 0.546798169613\n",
      "Epoch: 881 ; training loss: 0.546774804592\n",
      "Epoch: 882 ; training loss: 0.546751558781\n",
      "Epoch: 883 ; training loss: 0.546728312969\n",
      "Epoch: 884 ; training loss: 0.546705126762\n",
      "Epoch: 885 ; training loss: 0.54668211937\n",
      "Epoch: 886 ; training loss: 0.546658873558\n",
      "Epoch: 887 ; training loss: 0.546635806561\n",
      "Epoch: 888 ; training loss: 0.546612918377\n",
      "Epoch: 889 ; training loss: 0.546589791775\n",
      "Epoch: 890 ; training loss: 0.546566963196\n",
      "Epoch: 891 ; training loss: 0.546544075012\n",
      "Epoch: 892 ; training loss: 0.546521127224\n",
      "Epoch: 893 ; training loss: 0.546498298645\n",
      "Epoch: 894 ; training loss: 0.546475529671\n",
      "Epoch: 895 ; training loss: 0.546452879906\n",
      "Epoch: 896 ; training loss: 0.546430170536\n",
      "Epoch: 897 ; training loss: 0.546407580376\n",
      "Epoch: 898 ; training loss: 0.546384990215\n",
      "Epoch: 899 ; training loss: 0.546362280846\n",
      "Epoch: 900 ; training loss: 0.546339809895\n",
      "Epoch: 901 ; training loss: 0.546317279339\n",
      "Epoch: 902 ; training loss: 0.546294867992\n",
      "Epoch: 903 ; training loss: 0.546272456646\n",
      "Epoch: 904 ; training loss: 0.5462500453\n",
      "Epoch: 905 ; training loss: 0.546227693558\n",
      "Epoch: 906 ; training loss: 0.546205341816\n",
      "Epoch: 907 ; training loss: 0.546183109283\n",
      "Epoch: 908 ; training loss: 0.546160876751\n",
      "Epoch: 909 ; training loss: 0.546138763428\n",
      "Epoch: 910 ; training loss: 0.546116650105\n",
      "Epoch: 911 ; training loss: 0.546094596386\n",
      "Epoch: 912 ; training loss: 0.546072423458\n",
      "Epoch: 913 ; training loss: 0.546050429344\n",
      "Epoch: 914 ; training loss: 0.546028375626\n",
      "Epoch: 915 ; training loss: 0.546006441116\n",
      "Epoch: 916 ; training loss: 0.545984566212\n",
      "Epoch: 917 ; training loss: 0.545962691307\n",
      "Epoch: 918 ; training loss: 0.545940876007\n",
      "Epoch: 919 ; training loss: 0.545919060707\n",
      "Epoch: 920 ; training loss: 0.545897245407\n",
      "Epoch: 921 ; training loss: 0.545875608921\n",
      "Epoch: 922 ; training loss: 0.54585391283\n",
      "Epoch: 923 ; training loss: 0.54583209753\n",
      "Epoch: 924 ; training loss: 0.545810580254\n",
      "Epoch: 925 ; training loss: 0.545789003372\n",
      "Epoch: 926 ; training loss: 0.5457675457\n",
      "Epoch: 927 ; training loss: 0.545746028423\n",
      "Epoch: 928 ; training loss: 0.545724630356\n",
      "Epoch: 929 ; training loss: 0.545703172684\n",
      "Epoch: 930 ; training loss: 0.545681715012\n",
      "Epoch: 931 ; training loss: 0.545660436153\n",
      "Epoch: 932 ; training loss: 0.545639157295\n",
      "Epoch: 933 ; training loss: 0.545617759228\n",
      "Epoch: 934 ; training loss: 0.545596539974\n",
      "Epoch: 935 ; training loss: 0.54557543993\n",
      "Epoch: 936 ; training loss: 0.545554220676\n",
      "Epoch: 937 ; training loss: 0.545533120632\n",
      "Epoch: 938 ; training loss: 0.545511901379\n",
      "Epoch: 939 ; training loss: 0.545490860939\n",
      "Epoch: 940 ; training loss: 0.545469939709\n",
      "Epoch: 941 ; training loss: 0.545448899269\n",
      "Epoch: 942 ; training loss: 0.545427918434\n",
      "Epoch: 943 ; training loss: 0.545406997204\n",
      "Epoch: 944 ; training loss: 0.545386135578\n",
      "Epoch: 945 ; training loss: 0.545365273952\n",
      "Epoch: 946 ; training loss: 0.545344412327\n",
      "Epoch: 947 ; training loss: 0.545323729515\n",
      "Epoch: 948 ; training loss: 0.545302987099\n",
      "Epoch: 949 ; training loss: 0.545282185078\n",
      "Epoch: 950 ; training loss: 0.545261561871\n",
      "Epoch: 951 ; training loss: 0.545240938663\n",
      "Epoch: 952 ; training loss: 0.545220315456\n",
      "Epoch: 953 ; training loss: 0.545199811459\n",
      "Epoch: 954 ; training loss: 0.545179367065\n",
      "Epoch: 955 ; training loss: 0.545158803463\n",
      "Epoch: 956 ; training loss: 0.54513835907\n",
      "Epoch: 957 ; training loss: 0.545117914677\n",
      "Epoch: 958 ; training loss: 0.545097410679\n",
      "Epoch: 959 ; training loss: 0.545077264309\n",
      "Epoch: 960 ; training loss: 0.54505687952\n",
      "Epoch: 961 ; training loss: 0.545036435127\n",
      "Epoch: 962 ; training loss: 0.545016229153\n",
      "Epoch: 963 ; training loss: 0.544996023178\n",
      "Epoch: 964 ; training loss: 0.544975757599\n",
      "Epoch: 965 ; training loss: 0.544955730438\n",
      "Epoch: 966 ; training loss: 0.544935584068\n",
      "Epoch: 967 ; training loss: 0.544915437698\n",
      "Epoch: 968 ; training loss: 0.544895470142\n",
      "Epoch: 969 ; training loss: 0.544875383377\n",
      "Epoch: 970 ; training loss: 0.544855356216\n",
      "Epoch: 971 ; training loss: 0.54483538866\n",
      "Epoch: 972 ; training loss: 0.544815480709\n",
      "Epoch: 973 ; training loss: 0.544795691967\n",
      "Epoch: 974 ; training loss: 0.544775724411\n",
      "Epoch: 975 ; training loss: 0.544755876064\n",
      "Epoch: 976 ; training loss: 0.544736146927\n",
      "Epoch: 977 ; training loss: 0.544716358185\n",
      "Epoch: 978 ; training loss: 0.544696629047\n",
      "Epoch: 979 ; training loss: 0.54467689991\n",
      "Epoch: 980 ; training loss: 0.544657349586\n",
      "Epoch: 981 ; training loss: 0.544637560844\n",
      "Epoch: 982 ; training loss: 0.544617950916\n",
      "Epoch: 983 ; training loss: 0.544598460197\n",
      "Epoch: 984 ; training loss: 0.544578909874\n",
      "Epoch: 985 ; training loss: 0.544559419155\n",
      "Epoch: 986 ; training loss: 0.544539988041\n",
      "Epoch: 987 ; training loss: 0.544520497322\n",
      "Epoch: 988 ; training loss: 0.544501185417\n",
      "Epoch: 989 ; training loss: 0.544481813908\n",
      "Epoch: 990 ; training loss: 0.544462382793\n",
      "Epoch: 991 ; training loss: 0.544443070889\n",
      "Epoch: 992 ; training loss: 0.544423758984\n",
      "Epoch: 993 ; training loss: 0.544404566288\n",
      "Epoch: 994 ; training loss: 0.544385313988\n",
      "Epoch: 995 ; training loss: 0.544366121292\n",
      "Epoch: 996 ; training loss: 0.544346988201\n",
      "Epoch: 997 ; training loss: 0.54432785511\n",
      "Epoch: 998 ; training loss: 0.544308781624\n",
      "Epoch: 999 ; training loss: 0.544289708138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.76785713)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv('heart.csv')\n",
    "dataframe.head()\n",
    "n_epochs=10\n",
    "\n",
    "dummy = pd.get_dummies(dataframe['famhist'],prefix='famhist', drop_first=False)\n",
    "dataframe = pd.concat([dataframe,dummy], axis=1)\n",
    "dataframe.head()\n",
    "\n",
    "dataframe = dataframe.drop(['famhist'], axis=1)\n",
    "dataframe.head()\n",
    "\n",
    "learning_rate = 0.1\n",
    "split_frac = 0.8\n",
    "\n",
    "cols=['sbp','tobacco','ldl','adiposity','typea','obesity','alcohol','age']\n",
    "\n",
    "labels = dataframe['chd']\n",
    "\n",
    "for data in cols:\n",
    "    dataframe[data] = (dataframe[data])/ dataframe[data].max()\n",
    "    \n",
    "#print(data.head())\n",
    "#print(labels.shape)\n",
    "\n",
    "features = dataframe.drop(['chd'], axis=1)\n",
    "\n",
    "features, labels = np.array(features), np.array(labels)\n",
    "\n",
    "n_records = len(features)\n",
    "\n",
    "train_X, train_Y = features[:350], labels[:350]\n",
    "test_X, test_Y = features[350:], labels[350:]\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(tf.float32,[None, 10], name ='inputs' )\n",
    "labels = tf.placeholder(tf.int32, [None,], name='output')\n",
    "labels_one_hot = tf.one_hot(labels, 2)\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([10,2], stddev=0.1,mean = 0.0))\n",
    "b = tf.Variable(tf.zeros(2))       \n",
    "\n",
    "logits = tf.matmul(inputs,W) + b\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_one_hot)\n",
    "cost = tf.reduce_mean(entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={inputs:train_X, labels:train_Y})\n",
    "\n",
    "        print(\"Epoch: {0} ; training loss: {1}\".format(epoch, loss))\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_one_hot, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({inputs: test_X, labels: test_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
